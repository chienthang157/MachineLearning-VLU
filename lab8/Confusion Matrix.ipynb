{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB - Tính toán các chỉ số đánh giá từ Ma trận Nhầm lẫn\n",
    "\n",
    "## Giới thiệu\n",
    "Trong học máy và các bài toán phân loại, **ma trận nhầm lẫn** (Confusion Matrix) là một công cụ giúp đánh giá chất lượng của mô hình dự đoán. Ma trận này cho thấy kết quả dự đoán đúng và sai của mô hình.\n",
    "\n",
    "Một ma trận nhầm lẫn cơ bản cho bài toán phân loại nhị phân có dạng:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "TN & FP \\\\\n",
    "FN & TP \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Ý nghĩa các chỉ số trong ma trận nhầm lẫn\n",
    "- **True Negative (TN)**: Số lượng mẫu thực tế là **âm** (Negative) và mô hình cũng dự đoán là **âm**.\n",
    "- **False Positive (FP)**: Số lượng mẫu thực tế là **âm** nhưng mô hình lại dự đoán là **dương** (Positive). Đây còn gọi là **dương giả**.\n",
    "- **False Negative (FN)**: Số lượng mẫu thực tế là **dương** nhưng mô hình lại dự đoán là **âm**. Đây còn gọi là **âm giả**.\n",
    "- **True Positive (TP)**: Số lượng mẫu thực tế là **dương** và mô hình cũng dự đoán là **dương**.\n",
    "\n",
    "Các giá trị này có thể tính toán được nhiều chỉ số quan trọng, giúp đánh giá mô hình một cách toàn diện.\n",
    "\n",
    "## Các Chỉ số Đánh giá Hiệu quả Mô Hình\n",
    "\n",
    "### 1. Độ chính xác (Accuracy)\n",
    "**Độ chính xác** là tỷ lệ số dự đoán đúng trên tổng số mẫu. Chỉ số này cho biết mô hình dự đoán chính xác bao nhiêu phần trăm.\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "### 2. Độ nhạy (Recall) \n",
    "**Độ nhạy** hay còn gọi là **Tỷ lệ phát hiện dương** (Sensitivity) cho biết mô hình phát hiện đúng bao nhiêu phần trăm các mẫu dương. Độ nhạy đặc biệt quan trọng khi chúng ta muốn giảm thiểu số trường hợp **âm giả**.\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Ví dụ, nếu đang xây dựng mô hình chẩn đoán bệnh, Recall cao có nghĩa là mô hình không bỏ sót quá nhiều ca bệnh.\n",
    "\n",
    " Giải thích :\n",
    "- recall là tỷ lệ phát hiện đúng các mẫu dương.\n",
    "- Trong trường hợp chẩn đoán bệnh, các mẫu dương là các ca bệnh thực sự.\n",
    "- recall cao có nghĩa là mô hình phát hiện được hầu hết các ca bệnh, tức là không bỏ sót nhiều ca bệnh\n",
    "\n",
    "### 3. Độ đặc hiệu (Specificity)\n",
    "**Độ đặc hiệu** là tỷ lệ dự đoán đúng các mẫu âm trên tổng số các mẫu âm thực tế. Chỉ số này cho biết mô hình có khả năng nhận diện đúng các mẫu âm tốt như thế nào.\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "Ví dụ, trong mô hình phát hiện gian lận, Specificity cao tránh được các trường hợp báo động sai (dương giả).\n",
    "\n",
    "Giải thích: \n",
    "- Specificity là tỷ lệ dự đoán đúng các mẫu âm\n",
    "- Trong trường hợp phát hiện gian lận, các mẫu âm là các giao dịch không gian lận\n",
    "- Specificity cao có nghĩa là mô hình nhận diện đúng hầu hết các giao dịch không gian lận, tức là giảm thiểu các trường hợp báo động sai (False Positive)\n",
    "\n",
    "### 4. Giá trị dự đoán dương (Precision)\n",
    "**Precision** hay còn gọi là **Độ chính xác của các dự đoán dương** là tỷ lệ dự đoán đúng trong số tất cả các mẫu được dự đoán là dương. Precision đặc biệt quan trọng khi chi phí của việc dương giả cao.\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Ví dụ, trong mô hình phát hiện ung thư, Precision cao đảm bảo rằng những trường hợp bị đánh dấu là dương (bệnh nhân mắc ung thư) có khả năng mắc bệnh thực sự.\n",
    "\n",
    "Giải thích:\n",
    "- Precision là tỷ lệ dự đoán đúng trong số tất cả các mẫu được dự đoán là dương\n",
    "- Trong trường hợp phát hiện ung thư, các mẫu dương là các bệnh nhân được dự đoán mắc ung thư\n",
    "- Precision cao có nghĩa là hầu hết các bệnh nhân được dự đoán mắc ung thư thực sự mắc bệnh, tức là giảm thiểu các trường hợp dương giả (False Positive)\n",
    "\n",
    "### 5. F1-Score\n",
    "**F1 Score** là trung bình điều hòa giữa Precision và Recall. Chỉ số này hữu ích khi cần cân bằng giữa Recall và Precision, đặc biệt trong các bài toán mà một chỉ số cao hơn có thể dẫn đến một chỉ số khác bị giảm.\n",
    "$$\n",
    "F1 = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "F1 Score giúp đánh giá mô hình với dữ liệu không cân bằng, chẳng hạn như khi số lượng mẫu dương và âm chênh lệch đáng kể.\n",
    "\n",
    "## Tóm tắt\n",
    "| Chỉ số       | Công thức                                       | Ý nghĩa |\n",
    "|--------------|-------------------------------------------------|---------|\n",
    "| Accuracy     | $$\\frac{TP + TN}{TP + TN + FP + FN}$$           | Tỷ lệ dự đoán đúng trên tổng số mẫu |\n",
    "| Recall       | $$\\frac{TP}{TP + FN}$$                          | Tỷ lệ phát hiện đúng các mẫu dương |\n",
    "| Specificity  | $$\\frac{TN}{TN + FP}$$                          | Tỷ lệ phát hiện đúng các mẫu âm |\n",
    "| Precision    | $$\\frac{TP}{TP + FP}$$                          | Tỷ lệ các dự đoán dương chính xác |\n",
    "| F1-Score     | $$\\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$ | Cân bằng giữa Precision và Recall |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập nhẹ nhàng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cho ma trận nhầm lẫn sau: \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "50 & 10 \\\\\n",
    "5 & 30 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Định nghĩa ma trận nhầm lẫn\n",
    "2. Tính toán các chỉ số\n",
    "3. In kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[50 10]\n",
      " [5 30   ]]\n",
      "Accuracy: 0.84\n",
      "Recall: 0.86\n",
      "Specificity: 0.83\n",
      "Precision: 0.75\n",
      "F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa ma trận nhầm lẫn\n",
    "TN = 50\n",
    "FP = 10\n",
    "FN = 5\n",
    "TP = 30\n",
    "# Tính toán các chỉ số\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "recall = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "# In kết quả\n",
    "print(f\"Confusion Matrix:\\n[[{TN} {FP}]\\n [{FN} {TP}   ]]\")#Code here\n",
    "print(f\"Accuracy: {accuracy:.2f}\")#Code here\n",
    "print(f\"Recall: {recall:.2f}\")#Code here\n",
    "print(f\"Specificity: {specificity:.2f}\")#Code here\n",
    "print(f\"Precision: {precision:.2f}\")#Code here\n",
    "print(f\"F1 Score: { f1:.2f}\")#Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mức độ đặc hiệu và F1 Score cho thấy mô hình đã làm việc khá hiệu quả, nhưng để tối ưu hơn, có thể thử điều chỉnh các tham số hoặc thử các mô hình khác để cải thiện thêm độ chính xác.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nhận xét ở đây:\n",
    "\"Mô hình có độ chính xác và độ nhạy tốt, nhưng có thể cải thiện thêm về độ chính xác để giảm bớt các dự đoán dương tính giả.\"\n",
    "\"Mức độ đặc hiệu và F1 Score cho thấy mô hình đã làm việc khá hiệu quả, nhưng để tối ưu hơn, có thể thử điều chỉnh các tham số hoặc thử các mô hình khác để cải thiện thêm độ chính xác.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập nâng cao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sủ dụng Markfown để viết ra 4 chỉ số tính độ chính xác, và viết định nghĩa cho các công thức sau:\n",
    "\n",
    "1. Balanced Accuracy\n",
    "2. Matthews Correlation Coefficient (MCC)\n",
    "3. Fowlkes-Mallows Index (FMI)\n",
    "4. Bias\n",
    "\n",
    "### Ứng dụng 4 chỉ số này để tính toán cho bài tập nhẹ nhàng ở trên"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Chỉ số            | Công thức                                       | Định nghĩa |\n",
    "|-------------------|-------------------------------------------------|------------|\n",
    "| Balanced Accurary | $$\\text{Balanced Accuracy} = \\frac{\\text{Sensitivity} + \\text{Specificity}}{2}$$           | Balanced Accurary là một thước đo hiệu suất của mô hình, đặc biệt hữu ích khi làm việc với dữ liệu không cân bằng |\n",
    "| MCC               | $$\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$$                          | MCC là một thước đo dùng để đánh giá hiệu suất của mô hình phân loại nhị phân, đặc biệt hữu ích khi làm việc với dữ liệu không cân bằng |\n",
    "| FMI               | $$\\text{FMI} = \\sqrt{\\frac{TP}{TP + FP} \\cdot \\frac{TP}{TP + FN}}$$                          |  FMI là một thước đo được sử dụng để đánh giá hiệu suất của các mô hình phân cụm hoặc phân loại nhị phân . FMI đo mức độ tương đồng giữa các cụm dự đoán và các cụm thực tế|\n",
    "| Bias              | $$\\text{Bias} = \\text{Average Predicted Value} - \\text{Average Actual Value}$$                          |  Bias là một dạng sai số hệ thống trong mô hình. Bias thể hiện mức độ mà dự đoán trung bình của mô hình khác so với giá trị thực tế của dữ liệu |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy: 0.8295 (82.95%)\n",
      "Matthews Correlation Coefficient (MCC): 0.6746\n",
      "Fowlkes-Mallows Index (FMI): 0.8704\n",
      "Bias: 0.6842\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Ma trận nhầm lẫn\n",
    "confusion = np.array([[50, 10],\n",
    "                      [5, 30]])\n",
    "\n",
    "# Lấy các giá trị TP, TN, FP, FN\n",
    "TP = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "TN = confusion[1, 1]\n",
    "\n",
    "# 1. Balanced Accuracy\n",
    "recall = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "# 2. Matthews Correlation Coefficient (MCC)\n",
    "mcc_numerator = (TP * TN) - (FP * FN)\n",
    "mcc_denominator = np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "mcc = mcc_numerator / mcc_denominator if mcc_denominator != 0 else 0\n",
    "\n",
    "# 3. Fowlkes-Mallows Index (FMI)\n",
    "fmi = TP / np.sqrt((TP + FP) * (TP + FN))\n",
    "\n",
    "# 4. Bias\n",
    "bias = (TP + TN) / (TP + TN + FP + FN) - (FP + FN) / (TP + TN + FP + FN)\n",
    "\n",
    "# In kết quả\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.4f} ({balanced_accuracy * 100:.2f}%)\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "print(f\"Fowlkes-Mallows Index (FMI): {fmi:.4f}\")\n",
    "print(f\"Bias: {bias:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập vận dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature 1  Feature 2\n",
      "0   3.069402   5.612889\n",
      "1  -0.355127  -1.005306\n",
      "2   1.213291   2.977067\n",
      "3   1.521887   2.125014\n",
      "4   3.522842   2.539153\n",
      "Nhãn tương ứng: [1. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tạo dữ liệu giả định cho KNN\n",
    "np.random.seed(42)\n",
    "data_size = 1000\n",
    "# Tạo các đặc trưng ngẫu nhiên giữa các lớp\n",
    "X_class0 = np.random.multivariate_normal([2, 2], [[1.5, 0.75], [0.75, 1.5]], data_size // 2)\n",
    "X_class1 = np.random.multivariate_normal([4, 4], [[1.5, 0.75], [0.75, 1.5]], data_size // 2)\n",
    "X = np.vstack((X_class0, X_class1))\n",
    "y = np.hstack((np.zeros(data_size // 2), np.ones(data_size // 2)))\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra với test = 30 và random = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)#code here\n",
    "# Hiển thị một vài mẫu dữ liệu\n",
    "print(pd.DataFrame(X_train[:5], columns=[\"Feature 1\", \"Feature 2\"]))\n",
    "print(\"Nhãn tương ứng:\", y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))#code here\n",
    "def knn_predict(X_train, y_train, X_test, k=5):\n",
    "    y_pred = []\n",
    "    for test_point in X_test:\n",
    "        distances = [euclidean_distance(test_point, x) for x in X_train]\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = [y_train[i] for i in k_indices]\n",
    "        most_common = max(set(k_nearest_labels), key=k_nearest_labels.count)\n",
    "        y_pred.append(most_common)#code here\n",
    "    return np.array(y_pred)\n",
    "# Dự đoán trên tập kiểm tra với k = 5\n",
    "y_pred_knn = knn_predict(X_train, y_train, X_test, k=5)#code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model Evaluation:\n",
      "Confusion Matrix:\n",
      "[[116  34]\n",
      " [ 16 134]]\n",
      "Accuracy: 0.83\n",
      "Recall: 0.89\n",
      "Specificity: 0.77\n",
      "Precision: 0.80\n",
      "F1 Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa hàm confusion_matrix\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))#code here\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))#code here\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))#code here\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))#code here\n",
    "    return np.array([[TN, FP], [FN, TP]])#code here\n",
    "\n",
    "# Hàm tính toán và in các chỉ số\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)#code here\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0#code here\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0#code here\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0#code here\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0#code here\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"Specificity: {specificity:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "# Đánh giá mô hình KNN\n",
    "print(\"KNN Model Evaluation:\")\n",
    "y_pred_knn = knn_predict(X_train, y_train, X_test, k=5)\n",
    "evaluate_model(y_test, y_pred_knn)#code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập về nhà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tải tập dữ liệu Wine từ sklearn.datasets và chia tập dữ liệu theo tỷ lệ 70:30. Xây dựng mô hình KNN để phân loại dữ liệu. Sử dụng k = 5. Tính toán và in ra độ chính xác, recall, và precision của mô hình\n",
    "### Xây dựng website để trực quan hóa kết quả và độ chính xác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Tải tập dữ liệu Wine\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập thử nghiệm với tỷ lệ 70:30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n",
    "\n",
    "# Khởi tạo mô hình KNN với k = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán trên tập thử nghiệm\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác: 74.07%\n",
      "Báo cáo phân loại:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.89      0.89      0.89        19\n",
      "     class_1       0.75      0.71      0.73        21\n",
      "     class_2       0.53      0.57      0.55        14\n",
      "\n",
      "    accuracy                           0.74        54\n",
      "   macro avg       0.73      0.73      0.73        54\n",
      "weighted avg       0.74      0.74      0.74        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Code here\n",
    "\n",
    "# Tính độ chính xác\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Độ chính xác: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Tính toán báo cáo phân loại\n",
    "report = classification_report(y_test, y_pred, target_names=wine_data.target_names)\n",
    "print(\"Báo cáo phân loại:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
